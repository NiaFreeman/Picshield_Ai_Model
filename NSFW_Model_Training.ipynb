{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64512af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded797fd",
   "metadata": {},
   "source": [
    "## Step 1: Merge Datasets\n",
    "Combine the downloaded 60GB dataset with your Spicy_Ai images. We'll create a binary classifier:\n",
    "- **NSFW class**: porn + hentai + sexy categories + your Spicy_Ai images\n",
    "- **Safe class**: neutral + drawings categories + your non-sensitive images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b2d7d",
   "metadata": {},
   "source": [
    "## Step 0: Download 60GB NSFW Dataset (NSFWJS Training Data)\n",
    "This will clone the nsfw_data_scraper repo and automatically download ~60GB of labeled images from Reddit and other sources. **This will take several hours** - best to run overnight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb7bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the NSFW data scraper repository\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install required packages\n",
    "!pip install wget\n",
    "\n",
    "# Clone the repo if not already present\n",
    "scraper_dir = r'nsfw_data_scraper'\n",
    "if not os.path.exists(scraper_dir):\n",
    "    print('Cloning NSFW data scraper repository...')\n",
    "    !git clone https://github.com/alex000kim/nsfw_data_scraper.git\n",
    "    print('✓ Repository cloned!')\n",
    "else:\n",
    "    print('✓ Repository already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f0130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Ripme tool (required for scraping)\n",
    "ripme_path = r'nsfw_data_scraper\\ripme.jar'\n",
    "if not os.path.exists(ripme_path):\n",
    "    print('Downloading Ripme tool...')\n",
    "    import urllib.request\n",
    "    ripme_url = 'https://github.com/RipMeApp/ripme/releases/latest/download/ripme.jar'\n",
    "    urllib.request.urlretrieve(ripme_url, ripme_path)\n",
    "    print('✓ Ripme downloaded!')\n",
    "else:\n",
    "    print('✓ Ripme already downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the automated download scripts\n",
    "# WARNING: This downloads ~60GB of data and will take HOURS (3-6 hours depending on internet speed)\n",
    "# It scrapes images from Reddit and categorizes them into: porn, hentai, sexy, neutral, drawings\n",
    "\n",
    "print('Starting automated download...')\n",
    "print('This will download ~60GB of NSFW training data from multiple sources')\n",
    "print('Expected time: 3-6 hours')\n",
    "print('')\n",
    "\n",
    "os.chdir('nsfw_data_scraper')\n",
    "\n",
    "# Step 1: Get image URLs from source lists (fast - a few minutes)\n",
    "print('Step 1/3: Getting image URLs...')\n",
    "!bash scripts/1_get_urls.sh\n",
    "\n",
    "# Step 2: Download actual images from URLs (SLOW - several hours)\n",
    "print('Step 2/3: Downloading images (this takes the longest)...')\n",
    "!bash scripts/2_download_from_urls.sh\n",
    "\n",
    "# Step 3: Organize into train directory\n",
    "print('Step 3/3: Organizing images...')\n",
    "!bash scripts/5_create_train.sh\n",
    "\n",
    "os.chdir('..')\n",
    "print('')\n",
    "print('✓ Dataset download complete!')\n",
    "print('Images saved to: nsfw_data_scraper/data/train/')\n",
    "print('')\n",
    "print('Categories:')\n",
    "print('  - porn/')\n",
    "print('  - hentai/')\n",
    "print('  - sexy/')\n",
    "print('  - neutral/')\n",
    "print('  - drawings/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce2c02e",
   "metadata": {},
   "source": [
    "### Windows Alternative (if bash scripts don't work)\n",
    "If the bash scripts fail, use this Python-based downloader instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99622f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows-compatible Python downloader\n",
    "# This manually processes the URL lists and downloads images\n",
    "\n",
    "import urllib.request\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def download_from_url_file(url_file, output_dir):\n",
    "    \"\"\"Download images from a text file containing URLs\"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(url_file):\n",
    "        print(f'⚠ File not found: {url_file}')\n",
    "        return 0\n",
    "    \n",
    "    with open(url_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        urls = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f'Found {len(urls)} URLs in {os.path.basename(url_file)}')\n",
    "    downloaded = 0\n",
    "    \n",
    "    for i, url in enumerate(urls):\n",
    "        if i % 100 == 0:\n",
    "            print(f'  Progress: {i}/{len(urls)} ({downloaded} successful)')\n",
    "        \n",
    "        try:\n",
    "            # Extract filename from URL\n",
    "            filename = url.split('/')[-1]\n",
    "            if not any(filename.endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                filename += '.jpg'\n",
    "            \n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            # Download with timeout\n",
    "            urllib.request.urlretrieve(url, output_path)\n",
    "            downloaded += 1\n",
    "            time.sleep(0.1)  # Be nice to servers\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass  # Skip failed downloads\n",
    "    \n",
    "    print(f'✓ Downloaded {downloaded}/{len(urls)} images')\n",
    "    return downloaded\n",
    "\n",
    "# Process each category\n",
    "scraper_dir = r'nsfw_data_scraper'\n",
    "raw_data_dir = os.path.join(scraper_dir, 'raw_data')\n",
    "output_dir = os.path.join(scraper_dir, 'data', 'train')\n",
    "\n",
    "categories = {\n",
    "    'porn_urls.txt': 'porn',\n",
    "    'hentai_urls.txt': 'hentai',\n",
    "    'sexy_urls.txt': 'sexy',\n",
    "    'neutral_urls.txt': 'neutral',\n",
    "    'drawings_urls.txt': 'drawings'\n",
    "}\n",
    "\n",
    "print('Starting Windows-compatible download...')\n",
    "print('This will take several hours for 60GB of data\\n')\n",
    "\n",
    "total_downloaded = 0\n",
    "for url_file, category in categories.items():\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Downloading {category.upper()} category')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    url_path = os.path.join(raw_data_dir, url_file)\n",
    "    output_path = os.path.join(output_dir, category)\n",
    "    \n",
    "    count = download_from_url_file(url_path, output_path)\n",
    "    total_downloaded += count\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'DOWNLOAD COMPLETE')\n",
    "print(f'Total images downloaded: {total_downloaded:,}')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1aaa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to datasets\n",
    "downloaded_data = r'nsfw_data_scraper\\data\\train'\n",
    "spicy_source = r'C:\\Users\\Shado\\Downloads\\Spicy_Ai\\Spicy_Ai_Training\\samples'\n",
    "safe_source = r'dataset\\training\\nonsensitive'\n",
    "output_base = r'nsfw_dataset'\n",
    "\n",
    "# Create directory structure\n",
    "dirs = [\n",
    "    f'{output_base}/training/nsfw',\n",
    "    f'{output_base}/training/safe',\n",
    "    f'{output_base}/validation/nsfw',\n",
    "    f'{output_base}/validation/safe'\n",
    "]\n",
    "\n",
    "for d in dirs:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print('✓ Dataset directories created!')\n",
    "print('')\n",
    "print('Merging strategy:')\n",
    "print('  NSFW = porn + hentai + sexy + Spicy_Ai')\n",
    "print('  Safe = neutral + drawings + your non-sensitive images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all NSFW images from downloaded dataset + your Spicy_Ai\n",
    "import glob\n",
    "\n",
    "print('Collecting NSFW images...')\n",
    "\n",
    "nsfw_images = []\n",
    "\n",
    "# Add porn category\n",
    "porn_path = os.path.join(downloaded_data, 'porn')\n",
    "if os.path.exists(porn_path):\n",
    "    porn_imgs = glob.glob(os.path.join(porn_path, '*.*'))\n",
    "    nsfw_images.extend(porn_imgs)\n",
    "    print(f'  ✓ Porn: {len(porn_imgs)} images')\n",
    "\n",
    "# Add hentai category\n",
    "hentai_path = os.path.join(downloaded_data, 'hentai')\n",
    "if os.path.exists(hentai_path):\n",
    "    hentai_imgs = glob.glob(os.path.join(hentai_path, '*.*'))\n",
    "    nsfw_images.extend(hentai_imgs)\n",
    "    print(f'  ✓ Hentai: {len(hentai_imgs)} images')\n",
    "\n",
    "# Add sexy category\n",
    "sexy_path = os.path.join(downloaded_data, 'sexy')\n",
    "if os.path.exists(sexy_path):\n",
    "    sexy_imgs = glob.glob(os.path.join(sexy_path, '*.*'))\n",
    "    nsfw_images.extend(sexy_imgs)\n",
    "    print(f'  ✓ Sexy: {len(sexy_imgs)} images')\n",
    "\n",
    "# Add your Spicy_Ai images\n",
    "if os.path.exists(spicy_source):\n",
    "    spicy_imgs = glob.glob(os.path.join(spicy_source, '*.*'))\n",
    "    nsfw_images.extend(spicy_imgs)\n",
    "    print(f'  ✓ Spicy_Ai: {len(spicy_imgs)} images')\n",
    "\n",
    "print(f'\\nTotal NSFW images: {len(nsfw_images)}')\n",
    "\n",
    "# Shuffle and split 80/20\n",
    "np.random.shuffle(nsfw_images)\n",
    "split_idx = int(len(nsfw_images) * 0.8)\n",
    "train_nsfw = nsfw_images[:split_idx]\n",
    "val_nsfw = nsfw_images[split_idx:]\n",
    "\n",
    "# Copy training NSFW\n",
    "print(f'\\nCopying {len(train_nsfw)} training images...')\n",
    "for i, img in enumerate(train_nsfw):\n",
    "    if i % 1000 == 0:\n",
    "        print(f'  Progress: {i}/{len(train_nsfw)}')\n",
    "    try:\n",
    "        shutil.copy(img, f'{output_base}/training/nsfw/')\n",
    "    except Exception as e:\n",
    "        pass  # Skip corrupted images\n",
    "    \n",
    "# Copy validation NSFW\n",
    "print(f'Copying {len(val_nsfw)} validation images...')\n",
    "for img in val_nsfw:\n",
    "    try:\n",
    "        shutil.copy(img, f'{output_base}/validation/nsfw/')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "print(f'\\n✓ NSFW images copied: {len(train_nsfw)} training, {len(val_nsfw)} validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cdde13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all Safe images from downloaded dataset + your non-sensitive images\n",
    "print('Collecting Safe images...')\n",
    "\n",
    "safe_images = []\n",
    "\n",
    "# Add neutral category\n",
    "neutral_path = os.path.join(downloaded_data, 'neutral')\n",
    "if os.path.exists(neutral_path):\n",
    "    neutral_imgs = glob.glob(os.path.join(neutral_path, '*.*'))\n",
    "    safe_images.extend(neutral_imgs)\n",
    "    print(f'  ✓ Neutral: {len(neutral_imgs)} images')\n",
    "\n",
    "# Add drawings category (SFW anime/drawings)\n",
    "drawings_path = os.path.join(downloaded_data, 'drawings')\n",
    "if os.path.exists(drawings_path):\n",
    "    drawing_imgs = glob.glob(os.path.join(drawings_path, '*.*'))\n",
    "    safe_images.extend(drawing_imgs)\n",
    "    print(f'  ✓ Drawings: {len(drawing_imgs)} images')\n",
    "\n",
    "# Add your non-sensitive images\n",
    "if os.path.exists(safe_source):\n",
    "    your_safe_imgs = glob.glob(os.path.join(safe_source, '*.*'))\n",
    "    safe_images.extend(your_safe_imgs)\n",
    "    print(f'  ✓ Your non-sensitive: {len(your_safe_imgs)} images')\n",
    "\n",
    "print(f'\\nTotal Safe images: {len(safe_images)}')\n",
    "\n",
    "# Balance dataset (match NSFW count or use all)\n",
    "if len(safe_images) > len(nsfw_images):\n",
    "    print(f'Balancing dataset: using {len(nsfw_images)} safe images to match NSFW count')\n",
    "    safe_images = safe_images[:len(nsfw_images)]\n",
    "\n",
    "# Shuffle and split 80/20\n",
    "np.random.shuffle(safe_images)\n",
    "split_idx = int(len(safe_images) * 0.8)\n",
    "train_safe = safe_images[:split_idx]\n",
    "val_safe = safe_images[split_idx:]\n",
    "\n",
    "# Copy training Safe\n",
    "print(f'\\nCopying {len(train_safe)} training images...')\n",
    "for i, img in enumerate(train_safe):\n",
    "    if i % 1000 == 0:\n",
    "        print(f'  Progress: {i}/{len(train_safe)}')\n",
    "    try:\n",
    "        shutil.copy(img, f'{output_base}/training/safe/')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "# Copy validation Safe\n",
    "print(f'Copying {len(val_safe)} validation images...')\n",
    "for img in val_safe:\n",
    "    try:\n",
    "        shutil.copy(img, f'{output_base}/validation/safe/')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "print(f'\\n✓ Safe images copied: {len(train_safe)} training, {len(val_safe)} validation')\n",
    "print('')\n",
    "print('=' * 60)\n",
    "print('DATASET SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'Training:   {len(train_nsfw):,} NSFW + {len(train_safe):,} Safe = {len(train_nsfw) + len(train_safe):,} total')\n",
    "print(f'Validation: {len(val_nsfw):,} NSFW + {len(val_safe):,} Safe = {len(val_nsfw) + len(val_safe):,} total')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207bd05f",
   "metadata": {},
   "source": [
    "## Step 2: Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd469a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same preprocessing as your document model\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    f'{output_base}/training',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    f'{output_base}/validation',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "print('Data generators ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01eeb1",
   "metadata": {},
   "source": [
    "## Step 3: Build Model (Same Architecture as Your Document Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1752423",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ae029",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48efeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ec827",
   "metadata": {},
   "source": [
    "## Step 5: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057da297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in H5 format (for compatibility)\n",
    "model.save('models/nsfw_model.h5')\n",
    "print('Model saved as models/nsfw_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c21257",
   "metadata": {},
   "source": [
    "## Step 6: Convert to TensorFlow.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tensorflowjs converter if not already installed\n",
    "!pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d857ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TFJS format\n",
    "!tensorflowjs_converter --input_format=keras models/nsfw_model.h5 webapp/public/models/nsfw_model\n",
    "print('Model converted for web use!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6eca0",
   "metadata": {},
   "source": [
    "## Step 7: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a few images\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(150, 150))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "    \n",
    "    prediction = model.predict(img_array)[0][0]\n",
    "    label = 'NSFW' if prediction > 0.5 else 'Safe'\n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.title(f'{label} ({confidence*100:.1f}% confidence)')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test with an NSFW image\n",
    "test_nsfw = val_nsfw[0] if val_nsfw else None\n",
    "if test_nsfw:\n",
    "    print('Testing NSFW image:')\n",
    "    predict_image(test_nsfw)\n",
    "\n",
    "# Test with a safe image\n",
    "test_safe = val_safe[0] if val_safe else None\n",
    "if test_safe:\n",
    "    print('Testing safe image:')\n",
    "    predict_image(test_safe)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
